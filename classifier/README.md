# Intelli3 Classifier — Usage Guide

This module implements the **classification and recommendation layer** of the Intelli3 system.
It operates **after** the Intelli3 pipeline has generated **Multiple Intelligences (MI) vectors** for educational resources and is responsible for:

* Ranking documents by similarity to **cognitive profiles**
* Supporting **focused recommendations** by selected intelligences
* Evaluating results through **stability and effectiveness metrics**

The classifier is **fully decoupled** from ingestion, ontology instantiation, and vector generation.

---

## 1. Purpose

The Intelli3 Classifier operationalizes the final step of the Intelli3 computational model:

> using MI vectors to classify and recommend educational materials based on their proximity to reference cognitive profiles.

It supports:

* Proof-of-concept (PoC) evaluation
* Reproducible experiments
* Ranking stability analysis
* Quantitative validation against ground truth

---

## 2. Inputs and assumptions

### Required input

* An `output/` directory generated by the Intelli3 pipeline
* Each document folder must contain:

  * `instances_fragments_profile.ttl`
  * a valid MI vector (`onto:miVector`) or numeric equivalent

Example:

```
output/
├── activity_01/
│   └── instances_fragments_profile.ttl
├── activity_02/
│   └── instances_fragments_profile.ttl
└── batch_report.json
```

### MI vector format

Vectors are assumed to follow the **fixed order**:

```
[LING, LOG, ESP, MUS, CORP, INTER, INTRA, NAT]
```

All vectors must be normalized (ℓ1, ℓ2, or softmax).

---

## 3. Installation

The classifier is pure Python and has minimal dependencies.

### Python version

* Python **3.10+** recommended

### Dependencies

```bash
pip install numpy scipy
```

> `scipy` is required for Spearman correlation.

---

## 4. Module overview

```
classifier/
├── loader.py      # Reads MI vectors from output/
├── profiles.py    # Reference cognitive profiles
├── similarity.py  # Similarity and distance metrics
├── ranking.py     # Ranking and recommendation logic
├── tests.py       # Evaluation metrics (Spearman, F1, stability)
├── cli.py         # Command-line interface
└── README.md
```

---

## 5. Reference cognitive profiles

The module provides **predefined reference profiles**, aligned with the dissertation:

### Predominant profiles

* `P1_LING`
* `P2_LOG`
* `P3_ESP`
* `P4_CORP`
* `P5_MUS`
* `P6_INTER`
* `P7_INTRA`
* `P8_NAT`

### Balanced profiles

* `P9_HUM`   (humanities-oriented)
* `P10_TEC`  (science & technology)
* `P11_GLOB` (fully balanced)

All profiles are represented as MI vectors in the same space as documents.

---

## 6. Command-line usage

### Basic ranking

Ranks all documents by similarity to a reference profile.

```bash
python -m intelli3_classifier.cli \
  --output output \
  --profile P11_GLOB
```

Output:

```
activity_12   0.8421
activity_07   0.8014
activity_03   0.7799
...
```

Similarity is computed using **cosine similarity** by default.

---

### Ranking with intelligence focus

You may restrict the comparison to a **subset of intelligences**, simulating targeted pedagogical goals.

Example: science & technology focus

```bash
python -m intelli3_classifier.cli \
  --output output \
  --profile P10_TEC \
  --focus LOG,ESP,NAT
```

Behavior:

* Vectors are projected onto the selected dimensions
* Re-normalization is applied automatically
* Ranking reflects only the focused intelligences

---

## 7. Programmatic usage

### Loading document vectors

```python
from intelli3_classifier.loader import load_document_vectors

docs = load_document_vectors("output")
```

Returns:

```python
{
  "activity_01": [0.22, 0.08, 0.08, ...],
  "activity_02": [0.10, 0.30, 0.15, ...],
}
```

---

### Ranking documents

```python
from intelli3_classifier.ranking import rank_documents
from intelli3_classifier.profiles import PROFILES

ranking = rank_documents(
    docs,
    PROFILES["P11_GLOB"]
)
```

---

### Focused ranking

```python
ranking = rank_documents(
    docs,
    PROFILES["P10_TEC"],
    focus=["LOG", "ESP", "NAT"]
)
```

---

## 8. Evaluation and tests (PoC)

The module includes quantitative tests aligned with the PoC described in the dissertation.

### 8.1 Ranking stability (Spearman)

Measures ordinal stability between rankings.

```python
from intelli3_classifier.tests import ranking_stability

rho = ranking_stability(ranking_run1, ranking_run2)
```

* Uses **Spearman’s ρ**
* Values close to 1 indicate high stability

---

### 8.2 Effectiveness (F1-micro)

Compares predicted dominant intelligence against ground truth.

```python
from intelli3_classifier.tests import f1_micro

score = f1_micro(predicted_labels, true_labels)
```

* Suitable for multi-class MI evaluation
* Used in calibration and grid search

---

### 8.3 Vector stability

Repeated executions with fixed parameters should yield:

* high cosine similarity
* low ℓ2 distance between vectors

This supports reproducibility claims.

---

## 9. Typical experimental scenarios

The classifier supports:

* Recommendation for individual student profiles
* Recommendation for balanced or thematic curricula
* Sensitivity analysis to profile variations
* Stability analysis across runs
* Validation against labeled corpora (e.g., Antunes activities)

---

## 10. Design principles

* Fully decoupled from ontology and extraction layers
* Deterministic and reproducible
* Explicit parameters and transparent metrics
* Direct alignment with Design Science Research evaluation criteria

---

## 11. Scope and limitations

This classifier:

* Does **not** extract evidence
* Does **not** modify ontology instances
* Operates exclusively on generated MI vectors

Pedagogical effectiveness must be evaluated in subsequent naturalistic studies.

---
