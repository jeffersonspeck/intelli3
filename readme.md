# Intelli3

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Ontology](https://img.shields.io/badge/Ontology-RDF%2FOWL-informational)
![SHACL](https://img.shields.io/badge/Validation-SHACL-informational)
![LLM](https://img.shields.io/badge/LLM-Ollama%20Chat-informational)

Intelli3 is a **Python-based research prototype** that analyzes educational resources (primarily **text**) and produces an **explainable “Multiple Intelligences” profile** by combining:
- **Ontology-driven modeling** (OntoMI in RDF/TTL),
- **LLM-based annotation** (via Ollama chat),
- **Evidence extraction** (e.g., YAKE candidates + contextual classification),
- **Validation** (SHACL constraints).

This repository is part of my **M.Sc. research in Computer Science**, supervised by **Prof. Sidgley Camargo de Andrade** and **Prof. Clodis Boscarioli**.

---

## Repository Structure

### High-level directories

- **`docs/`**  
  Documentation files (module-by-module readmes and technical notes).  
  The main index is: **[`docs/README.md`](docs/README.md)**

- **`PoC/`**  
  Proof-of-concept materials demonstrating the pipeline running end-to-end.

- **`source/`**  
  Input materials to be analyzed (texts and other supported resources you want the pipeline to process).

- **`output/`**  
  Pipeline outputs (artifacts generated by each stage: intermediate JSON/RDF, final vectors, reports, etc.).

### Current root files (from your project snapshot)

```text
.
├── docs/
├── output/
├── PoC/
├── source/
├── evidences_api.py
├── evidences_api_helpers.py
├── ontomi.ttl
├── readme.md
├── run_batch.py
├── s1_ingest.py
├── s2_instantiate.py
├── s3_evidences.py
├── s4_activations.py
├── s5_profile_vector.py
├── s6_tests.py
└── s7_shacl_validate.py
````

---

## Pipeline Overview (Stages)

The pipeline is organized in sequential scripts:

1. **`s1_ingest.py`** — ingestion/preparation of raw inputs from `source/`
2. **`s2_instantiate.py`** — creates initial structured instances (often RDF entities / base metadata)
3. **`s3_evidences.py`** — extracts evidence candidates and assigns MI-related signals (calls `evidences_api.py`)
4. **`s4_activations.py`** — consolidates evidence into activation events (primary/secondary signals, scoring, etc.)
5. **`s5_profile_vector.py`** — computes the final MI vector/profile from activations
6. **`s6_tests.py`** — diagnostic/verification routines for experiments
7. **`s7_shacl_validate.py`** — validates generated RDF against SHACL shapes

`run_batch.py` is the orchestrator that typically runs stages in order over a batch of inputs.

---

## Documentation (Module Readmes)

All detailed docs live under **`docs/`**. Start here: **[`docs/README.md`](docs/README.md)**.

Planned per-file documentation pages (links you can keep in the main README):

* Core / LLM Evidence Layer

  * [`docs/evidences_api.md`](docs/evidences_api.md) — LLM-based evidence annotation (YAKE + Ollama + scoring)
  * [`docs/evidences_api_helpers.md`](docs/evidences_api_helpers.md) — helper utilities used by the evidence layer

* Ontology

  * [`docs/ontomi.md`](docs/ontomi.md) — OntoMI ontology file (`ontomi.ttl`), namespaces, main classes/properties

* Orchestration

  * [`docs/run_batch.md`](docs/run_batch.md) — batch execution, parameters, expected I/O

* Pipeline Stages

  * [`docs/s1_ingest.md`](docs/s1_ingest.md)
  * [`docs/s2_instantiate.md`](docs/s2_instantiate.md)
  * [`docs/s3_evidences.md`](docs/s3_evidences.md)
  * [`docs/s4_activations.md`](docs/s4_activations.md)
  * [`docs/s5_profile_vector.md`](docs/s5_profile_vector.md)
  * [`docs/s6_tests.md`](docs/s6_tests.md)
  * [`docs/s7_shacl_validate.md`](docs/s7_shacl_validate.md)

---

## Directory Conventions

* Put your raw materials in: **`source/`**
* Run the pipeline / batch scripts
* Collect results in: **`output/`**
* Keep demonstration runs and reproducible examples in: **`PoC/`**
* Keep all technical documentation in: **`docs/`**

---

````md
---

## How to Use (Batch Pipeline)

Intelli3 is designed to be run in **batch mode** over `.txt` files placed in `source/`. The main entry point is:

- **`run_batch.py`** — orchestrates steps **S1 → S7** and writes outputs per document under `output/`.

### What happens by default

When you run:

```bash
python run_batch.py
````

the batch runner will:

1. Read all `.txt` files from `./source/`
2. Create an output folder per input: `./output/<slug_of_filename>/`
3. Execute the enabled steps (S1 → S7), respecting dependencies
4. Write step artifacts + telemetry (`run_log.json`)
5. Write a global `batch_report.json` under `./output/`

> **Step dependency rule (important):**
>
> * If **S2** is disabled, **S3..S7** won’t run.
> * If **S3** is disabled, **S4..S7** won’t run.
> * If **S4** is disabled, **S5..S7** won’t run.
> * If **S5** is disabled, **S6..S7** won’t run.
> * If **S6** is disabled, **S7** won’t run.

---

## Installation

### 1) Python

* **Python 3.10+** (recommended)

Create and activate a virtual environment:

```bash
python -m venv .venv
# Windows (PowerShell)
.venv\Scripts\Activate.ps1
# Linux/macOS
source .venv/bin/activate
```

### 2) Core dependencies

At minimum (typical for S2+ / RDF + SHACL):

```bash
pip install rdflib pyshacl
```

For ingest and text processing (S1) and evidence extraction (S3), you will likely also need:

```bash
pip install spacy unidecode yake ftfy clean-text requests
```

> **Note:** S1 uses your local module **`intelli3text`** (project-specific).
> Install it as your environment requires (e.g., editable install from your own repo):

```bash
pip install -e path/to/intelli3text
```

### 3) spaCy models (if S1 is enabled)

Install at least one spaCy model (choose one size you prefer):

```bash
python -m spacy download en_core_web_md
python -m spacy download pt_core_news_md
python -m spacy download es_core_news_md
```

If you use `--nlp-size lg`, install `*_lg` models instead.

### 4) LLM runtime (if S3 uses an LLM)

If your `evidences_api.py` uses Ollama:

1. Install Ollama (OS-specific)
2. Ensure it is running on the configured host (default is usually `http://127.0.0.1:11434`)
3. Pull your target model, e.g.:

```bash
ollama pull mistral:7b-instruct
```

---

## Quick Start

### 1) Put text files in `source/`

```bash
mkdir -p source
# put .txt files into ./source/
```

### 2) Ensure the ontology is available

`ontomi.ttl` must exist at project root (or pass `--onto <path>`).

### 3) Run the pipeline

```bash
python run_batch.py
```

---

## CLI Usage

### Basic

```bash
python run_batch.py --source-dir source --out-dir output
```

### Turn steps on/off

* `--no-s2` : disable S2 (and therefore S3..S7)
* `--no-s3` : disable S3 (and therefore S4..S7)
* `--no-s4` : disable S4 (and therefore S5..S7)
* `--no-s5` : disable S5 (and therefore S6..S7)
* `--no-s6` : disable S6 (and therefore S7)
* `--no-s7` : disable S7

Examples:

Run **only S1** (ingest JSON):

```bash
python run_batch.py --no-s2
```

Run **S1 + S2 only**:

```bash
python run_batch.py --no-s3
```

Run full pipeline but disable SHACL validation:

```bash
python run_batch.py --no-s7
```

---

## Full Parameter Reference

Below is a grouped reference for the most relevant parameters supported by the batch runner.

> Defaults shown here reflect the *expected* defaults described in the batch documentation.
> (If a default differs in code, the code is the source of truth.)

---

### Paths

* `--source-dir` (default: `source`)
  Folder containing `.txt` input documents.

* `--out-dir` (default: `output`)
  Output root folder. A subfolder is created per document.

---

### S1 — Ingest (core options)

These parameters affect how the raw text is cleaned, segmented, and annotated.

* `--lang-hint` (default: `None`, choices: `pt|en|es`)
  Forces/limits language handling to a single language. Useful if inputs are known to be only PT/EN/ES.

* `--cleaners` (default: `ftfy,clean_text,pdf_breaks`)
  Comma-separated list of cleaner passes (typically forwarded to `intelli3text`).
  Typical cleaners:

  * `ftfy`: fixes broken unicode / encoding artifacts
  * `clean_text`: general cleanup (whitespace, control chars, etc.)
  * `pdf_breaks`: mitigates line-break artifacts common in PDF exports

* `--lid-primary` (default: `fasttext`)
  Primary language identification engine used in S1.

* `--lid-fallback` (default: `none`)
  Optional fallback LID engine (e.g., `cld3`) or `none`.

* `--languages` (default: `pt,en,es`)
  Comma-separated list of supported languages for LID and downstream processing.

* `--nlp-size` (default: `lg`, choices: `lg|md|sm`)
  Preferred spaCy model size:

  * `sm`: lightweight, fastest
  * `md`: balanced
  * `lg`: best accuracy, heavier

---

### S1 — Segmentation and fallback splitting

These parameters control paragraph segmentation and resegmentation fallback.

* `--paragraph-min-chars` (default: `30`)
  Minimum paragraph size kept as a valid paragraph.

* `--lid-min-chars` (default: `60`)
  Minimum paragraph length required to run language detection.

* `--split-max-chars` (default: `900`)
  Target chunk size when resegmenting long paragraphs.

* `--split-min-chars` (default: `120`)
  Minimum chunk size during resegmentation (very short chunks are merged).

* `--no-resegment`
  Disables the fallback resegmentation when a huge paragraph is detected.

---

### S1 — Title/Abstract detection

* `--force-title`
  Forces marking a paragraph as title/abstract (even if heuristics are uncertain).

* `--title-scan-k` (default: `3`)
  How many initial paragraphs are scanned as title/abstract candidates.

* `--title-max-chars` (default: `160`)
  Maximum number of characters for a paragraph to qualify as a title candidate.

---

### S2 — RDF instantiation

* `--onto` (default: `ontomi.ttl`)
  Path to the OntoMI ontology (TTL). Required for S2+.

* `--base-ns` (default: `None`)
  Base namespace for generated instances.
  If omitted, the pipeline may fall back to a namespace derived from `doc_id#`.

* `--s2-graph` (default: `full`)
  Graph content mode:

  * `full`: ontology + instances together
  * `instances`: instances only (lighter TTL)
  * `instances+imports`: instances plus `owl:imports` links

---

### S3 — Evidences

* `--s3-no-llm`
  Attempts to run S3 without LLM usage.
  Only works if your `evidences_api.py` implements a non-LLM fallback.

* `--s3-no-evokes`
  Prevents creating `onto:evokesIntelligence` links for evidences, even if available.
  Useful for ablation studies or to avoid biasing later stages.

---

### S4 — Activations (threshold + weights)

S4 computes activation scores per fragment/intelligence using evidence types and weights.

* `--theta` (default: `0.75`)
  Relative threshold for linking *secondary* intelligences per fragment.
  Higher = fewer secondaries; lower = more secondaries.

* `--w-keyword` (default: `1.00`)
  Weight for **Keyword** evidences.

* `--w-context` (default: `1.25`)
  Weight for **ContextObject** evidences.

* `--w-strategy` (default: `1.10`)
  Weight for **DiscursiveStrategy** evidences.

---

### S5 — MI profile vector

S5 builds profile vectors from activations, for fragments and/or documents.

* `--s5-scope` (default: `both`)
  Vector scope:

  * `document`: only a document-level vector
  * `fragment`: only per-fragment vectors
  * `both`: both document + fragment vectors

* `--s5-norm` (default: `l1`)
  Normalization:

  * `l1`: L1 normalization
  * `l2`: L2 normalization
  * `softmax`: softmax distribution (requires `--s5-tau`)

* `--s5-tau` (default: `1.0`)
  Softmax temperature (only used when `--s5-norm=softmax`).
  Lower values make distributions “sharper”; higher values make them “flatter”.

* `--alpha-title` (default: `1.30`)
  Weight multiplier for title/abstract paragraphs.

* `--alpha-body` (default: `1.00`)
  Weight multiplier for body paragraphs.

* `--s5-vec-places` (default: `4`)
  Decimal places written in vector scores.

* `--s5-no-mi-vector-string`
  Disables writing the percent-string representation (`onto:miVector`).

---

### S7 — SHACL validation

* `--s7-inference` (default: `rdfs`)
  Inference mode for pySHACL:

  * `none`
  * `rdfs`
  * `owlrl`

---

## Outputs (per document)

Each input document generates a folder:

`output/<doc_slug>/`

Typical artifacts:

* `s1_output.json`
* `instances_fragments.ttl`
* `instances_fragments_evidences.ttl`
* `evidences_payload.json`
* `scores_by_fragment.json`
* `instances_fragments_activations.ttl`
* `instances_fragments_profile.ttl`
* `cq1_evoked.txt`
* `cq2_elements_by_intel.txt`
* `cq3_top_intelligence.txt`
* `shacl_report.ttl`
* `shacl_report.txt`
* `run_log.json`

At output root:

* `output/batch_report.json`

---

## Examples

### Full pipeline, default settings

```bash
python run_batch.py
```

### Full pipeline, but disable LLM in S3

```bash
python run_batch.py --s3-no-llm
```

### Make the secondary linking stricter (fewer secondaries)

```bash
python run_batch.py --theta 0.90
```

### Use softmax normalization

```bash
python run_batch.py --s5-norm softmax --s5-tau 0.7
```

### Instances-only RDF graph (lighter TTL)

```bash
python run_batch.py --s2-graph instances
```

### Fully

```bash
python run_batch.py --source-dir source --out-dir output --lang-hint pt   --cleaners "ftfy,clean_text,pdf_breaks"   --lid-primary fasttext --lid-fallback none --languages "pt,en,es"   --nlp-size md --paragraph-min-chars 60 --lid-min-chars 60   --split-max-chars 900 --split-min-chars 250   --force-title --title-scan-k 5 --title-max-chars 160   --onto ontomi.ttl --s2-graph instances   --theta 0.75 --w-keyword 1.00 --w-context 1.25 --w-strategy 1.10   --s5-scope both --s5-norm l1 --s5-tau 1.0   --alpha-title 1.30 --alpha-body 1.00 --s5-vec-places 4   --s7-inference rdfs
```

### More lines

```bash
python run_batch.py --source-dir source --out-dir output --lang-hint pt   --cleaners "ftfy,clean_text,pdf_breaks"   --lid-primary fasttext --lid-fallback none --languages "pt,en,es"   --nlp-size md --paragraph-min-chars 20 --lid-min-chars 30   --split-max-chars 300 --split-min-chars 80   --force-title --title-scan-k 5 --title-max-chars 160   --onto ontomi.ttl --s2-graph instances   --theta 0.75 --w-keyword 1.00 --w-context 1.25 --w-strategy 1.10   --s5-scope both --s5-norm l1 --s5-tau 1.0   --alpha-title 1.30 --alpha-body 1.00 --s5-vec-places 4   --s7-inference rdfs
```

---

## Outputs and telemetry

For each document:

* `run_log.json` contains durations and per-step metrics
* `batch_report.json` aggregates outcomes for the whole run

Failures:

* If processing a file fails, its output folder will contain `s1_error.txt` and the batch continues.

---

## Troubleshooting

### “ontologia não encontrada”

If S2 is enabled, `--onto` must exist (default: `ontomi.ttl`).

### S3 fails with LLM / Ollama errors

If your `evidences_api.py` uses an LLM:

* Ensure the LLM runtime is up
* Ensure the target model is pulled/available
* Or run with `--s3-no-llm` (only if your evidences implementation supports it)

### SHACL doesn’t conform

Check `shacl_report.txt` and `shacl_report.ttl` in the document folder.

---
